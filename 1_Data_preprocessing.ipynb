{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the datasets in 'df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country      0\n",
       "Age          1\n",
       "Salary       1\n",
       "Purchased    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data.csv\")\n",
    "#checking for the null values.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here now, X and Y are extracted as the independent and dependent variable set. You can use either iloc, loc, or ix for this. \".Values\" added after the loc, is for the removal of the indexes and creats a 2 dimensional matrix, otherwise X/Y would have been a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, :'Salary'].values\n",
    "Y = df.loc[:, ['Purchased']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see how to handle the missing data. We can either remove the rows with the missing data, but it will lead to crucial loss of info. We can replace the NaN values with the mean of the other values, using Imputer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n",
      "Note, Mean of Salary is:\n",
      "63777.7777778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "##creating object of the class:\n",
    "impt = Imputer(missing_values = \"NaN\", strategy = \"mean\", axis = 0)\n",
    "f = impt.fit(X[:, 1:3])\n",
    "X[:, 1:3] = f.transform(X[:, 1:3])\n",
    "print X\n",
    "##for check\n",
    "print \"Note, Mean of Salary is:\" \n",
    "print df.Salary.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to deal with the categorical variables. Like the Country, and Purchased variable have categorical variable. Although, the no. from below code are labeled as 0, 1, or 2, but the code will think that, 2>0 or Spain> France, thus it does not makes sense. We will prevent this by making **dummy variables**, this means that we will be using a column for each categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 44.0, 72000.0],\n",
       "       [2, 27.0, 48000.0],\n",
       "       [1, 30.0, 54000.0],\n",
       "       [2, 38.0, 61000.0],\n",
       "       [1, 40.0, 63777.77777777778],\n",
       "       [0, 35.0, 58000.0],\n",
       "       [2, 38.77777777777778, 52000.0],\n",
       "       [0, 48.0, 79000.0],\n",
       "       [1, 50.0, 83000.0],\n",
       "       [0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lbX = LabelEncoder()\n",
    "f = lbX.fit(X[:, 0])\n",
    "X[:, 0] = f.transform(X[:, 0])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##now creating dummy variable\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(categorical_features = [0])\n",
    "f = ohe.fit(X)\n",
    "#toarray is used to convert a sparse matrix to a dense matrix\n",
    "X = f.transform(X).toarray()\n",
    "print X.shape\n",
    "#Now for Y:\n",
    "lbY = LabelEncoder()\n",
    "f = lbY.fit(Y)\n",
    "Y = f.transform(Y)\n",
    "len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to split the data into a training set and the test set to train and validate our model or measure the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "trainx, testx, trainy, testy = train_test_split(X, Y, test_size = 0.2)\n",
    "trainx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to go with the feature scaling part! We have numerical no. on a completely diff scales, say compare Age and Salary. This causes issues in ML models. Lot of ML models are based on euclidian distance, or dist between 2 points. Since Salary has much wider range of Values, the dist will be dominated by Salary, undermining the Age vector.\n",
    "\n",
    "Another question is: should we scale the dummy variables. It depends on the context, scaling surely increases the accuracy, but decreases the interpretation.\n",
    "\n",
    "Feature scaling also reduces runtime process of several models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.290994</td>\n",
       "      <td>-0.577350</td>\n",
       "      <td>-0.774597</td>\n",
       "      <td>0.991953</td>\n",
       "      <td>1.130898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.774597</td>\n",
       "      <td>-0.577350</td>\n",
       "      <td>1.290994</td>\n",
       "      <td>-1.641779</td>\n",
       "      <td>-1.330301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.774597</td>\n",
       "      <td>-0.577350</td>\n",
       "      <td>1.290994</td>\n",
       "      <td>0.182898</td>\n",
       "      <td>-0.920101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.774597</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>-0.774597</td>\n",
       "      <td>0.372251</td>\n",
       "      <td>0.287710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.290994</td>\n",
       "      <td>-0.577350</td>\n",
       "      <td>-0.774597</td>\n",
       "      <td>-0.402376</td>\n",
       "      <td>-0.304801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.774597</td>\n",
       "      <td>-0.577350</td>\n",
       "      <td>1.290994</td>\n",
       "      <td>0.062401</td>\n",
       "      <td>0.002849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.290994</td>\n",
       "      <td>-0.577350</td>\n",
       "      <td>-0.774597</td>\n",
       "      <td>1.611655</td>\n",
       "      <td>1.848748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.774597</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>-0.774597</td>\n",
       "      <td>-1.177003</td>\n",
       "      <td>-0.715001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "0  1.290994 -0.577350 -0.774597  0.991953  1.130898\n",
       "1 -0.774597 -0.577350  1.290994 -1.641779 -1.330301\n",
       "2 -0.774597 -0.577350  1.290994  0.182898 -0.920101\n",
       "3 -0.774597  1.732051 -0.774597  0.372251  0.287710\n",
       "4  1.290994 -0.577350 -0.774597 -0.402376 -0.304801\n",
       "5 -0.774597 -0.577350  1.290994  0.062401  0.002849\n",
       "6  1.290994 -0.577350 -0.774597  1.611655  1.848748\n",
       "7 -0.774597  1.732051 -0.774597 -1.177003 -0.715001"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "trainx = ss.fit_transform(trainx)\n",
    "testx =ss.transform(testx)\n",
    "t = pd.DataFrame(trainx)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
